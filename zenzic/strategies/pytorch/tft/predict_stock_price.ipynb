{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\xzhu\\\\Documents\\\\GitHub\\\\trading')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0362b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import zenzic.strategies.pytorch.tft.data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1d8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "PRED_LEN = 32\n",
    "START_DATE = '2010-01-01'\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0058cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read prices: 100%|██████████| 498/498 [01:08<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "prices = dp.sp500_prices(start_date=START_DATE)\n",
    "date_features = dp.date_features(prices)\n",
    "prices = prices.join(date_features, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1914e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = dp.split_data(prices, SEQ_LEN, PRED_LEN, 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940afd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TimeSeriesDataSet(\n",
    "    df_train,\n",
    "    time_idx='Date_idx',\n",
    "    target='Close',\n",
    "    group_ids=['Symbol'],\n",
    "    max_encoder_length=SEQ_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    time_varying_known_reals=[\n",
    "        'Date_idx',\n",
    "        'Day_of_year',\n",
    "        'Day_of_week',\n",
    "        'Month',\n",
    "        'Day'],\n",
    "    time_varying_unknown_reals=[\n",
    "        'Open',\n",
    "        'High',\n",
    "        'Low',\n",
    "        'Close'\n",
    "    ],\n",
    "    target_normalizer=TorchNormalizer(),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=False,\n",
    "    add_encoder_length=False,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "test_data = TimeSeriesDataSet.from_dataset(train_data, df_test, predict=False, stop_randomization=True)\n",
    "\n",
    "train_dataloader = train_data.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "test_dataloader = test_data.to_dataloader(train=False, batch_size=BATCH_SIZE, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4fb161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 24.5k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_data,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer=MADGRAD\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d704386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1079: LightningDeprecationWarning: `trainer.tune(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.tune(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "Global seed set to 42\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:26<00:00,  3.87it/s]Restoring states from the checkpoint path at c:\\Users\\xzhu\\Documents\\GitHub\\trading\\zenzic\\strategies\\pytorch\\tft\\lr_find_temp_model_a32111e3-1ebe-4490-ad14-2094f8aaa43f.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:26<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 0.011481536214968821\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu\\AppData\\Local\\Temp/ipykernel_2264/3771350729.py:12: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=test_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5aa6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 24.5k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.01,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_data,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    "    optimizer=MADGRAD\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb7c2f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 160   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 6.6 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "24.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.5 K    Total params\n",
      "0.098     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 11708/11709 [28:21<00:00,  6.88it/s, loss=0.00375, v_num=3, train_loss_step=0.00356]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 90/11709 [00:16<36:07,  5.36it/s, loss=0.00327, v_num=3, train_loss_step=0.00369, val_loss=0.00503, train_loss_epoch=0.0046]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 91/11709 [00:32<1:09:56,  2.77it/s, loss=0.00327, v_num=3, train_loss_step=0.00369, val_loss=0.00503, train_loss_epoch=0.0046]"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b98783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 08:04:41,059]\u001b[0m A new study created in memory with name: no-name-52721de8-e3ae-48fb-90cf-00968de0afb9\u001b[0m\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\u001b[33m[W 2022-01-27 08:04:41,078]\u001b[0m Trial 0 failed because of the following error: TypeError(\"__init__() got an unexpected keyword argument 'logging_level'\")\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\tuning.py\", line 157, in objective\n",
      "    model = TemporalFusionTransformer.from_dataset(\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py\", line 351, in from_dataset\n",
      "    return super().from_dataset(\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py\", line 1437, in from_dataset\n",
      "    return super().from_dataset(dataset, **new_kwargs)\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py\", line 969, in from_dataset\n",
      "    net = cls(**kwargs)\n",
      "  File \"C:\\Users\\xzhu\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py\", line 140, in __init__\n",
      "    super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "TypeError: __init__() got an unexpected keyword argument 'logging_level'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'logging_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2264/1348117775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# create study\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m study = optimize_hyperparameters(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\tuning.py\u001b[0m in \u001b[0;36moptimize_hyperparameters\u001b[1;34m(train_dataloader, val_dataloader, model_path, max_epochs, n_trials, timeout, gradient_clip_val_range, hidden_size_range, hidden_continuous_size_range, attention_head_size_range, dropout_range, learning_rate_range, use_learning_rate_finder, trainer_kwargs, log_dir, study, verbose, pruner, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpruner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\tuning.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hidden_size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mhidden_size_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         model = TemporalFusionTransformer.from_dataset(\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dropout\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdropout_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[1;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;31m# create class and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         return super().from_dataset(\n\u001b[0m\u001b[0;32m    352\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_encoder_known_variable_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallowed_encoder_known_variable_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[1;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[0;32m   1435\u001b[0m         )\n\u001b[0;32m   1436\u001b[0m         \u001b[0mnew_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1437\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m     def calculate_prediction_actual_by_variable(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[1;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"output_transformer\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output_transformer\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_target\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hidden_size, lstm_layers, dropout, output_size, loss, attention_head_size, max_encoder_length, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, x_reals, x_categoricals, hidden_continuous_size, hidden_continuous_sizes, embedding_sizes, embedding_paddings, embedding_labels, learning_rate, log_interval, log_val_interval, log_gradient_flow, reduce_on_plateau_patience, monotone_constaints, share_single_variable_networks, logging_metrics, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# store loss function separately as it is a module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLightningMetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Loss has to be a PyTorch Lightning `Metric`\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogging_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# processing inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'logging_level'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "    verbose=True,\n",
    "    optimizer=MADGRAD\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f38e2aa8a8b03d6974a44e7e1a228e00fab59178e958adcb91d243b1e30ae2cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
